{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AsEmRwxeJb4f",
        "outputId": "fa2d2b56-d16b-42f2-b3a0-8891fee30f9e"
      },
      "outputs": [],
      "source": [
        "import ssl\n",
        "import urllib.request\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "import csv\n",
        "import time\n",
        "from os import path, makedirs \n",
        "\n",
        "# Ignore SSL certificate errors\n",
        "ctx = ssl.create_default_context()\n",
        "ctx.check_hostname = False\n",
        "ctx.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "# Finding the number of events in the catalog\n",
        "main_url = \"https://www.endurance-data.com/en/competitions/1/\"\n",
        "main_html = urllib.request.urlopen(main_url, context=ctx).read()\n",
        "main_soup = BeautifulSoup(main_html, 'html.parser')\n",
        "pagination = main_soup(class_='page-item')\n",
        "num_pages = int(pagination[3].text)\n",
        "dir_url = \"https://www.endurance-data.com/en/competitions/{}/\"\n",
        "complete_race_info = []\n",
        "\n",
        "# Extracting information from each race\n",
        "for i in range(1, num_pages + 1):\n",
        "    dir_html = urllib.request.urlopen(dir_url.format(i), context=ctx).read()\n",
        "    dir_soup = BeautifulSoup(dir_html, 'html.parser')\n",
        "    races = dir_soup.find_all(class_='cursor-pointer')\n",
        "\n",
        "    for race in races:\n",
        "        race_stats = []\n",
        "        race_info = race.find_all('td')\n",
        "        race_results = race.find_all('a')\n",
        "        for info in race_info:\n",
        "            try:\n",
        "                if not re.match('[\\n]+', info.text):\n",
        "                    race_stats.append(info.text)\n",
        "            except Exception:\n",
        "                continue\n",
        "        race_stats.append(race_results[2]['href'])\n",
        "        complete_race_info.append(race_stats)\n",
        "\n",
        "all_races_dataframe = pd.DataFrame.from_records(complete_race_info)\n",
        "all_races_dataframe.rename(columns={\n",
        "    0: 'Event',\n",
        "    1: 'Date',\n",
        "    2: 'Location',\n",
        "    3: 'Athletes',\n",
        "    4: 'Results'\n",
        "}, inplace=True)\n",
        "all_races_dataframe.to_csv('./2015_present.csv', index=False)\n",
        "files.download('2015_present.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Svl7i8Z-0IFM",
        "outputId": "bf62afc1-1960-4413-b41c-852eac903516"
      },
      "outputs": [],
      "source": [
        "# Ignore SSL certificate errors\n",
        "ctx = ssl.create_default_context()\n",
        "ctx.check_hostname = False\n",
        "ctx.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "# Getting time to find program execution time\n",
        "start_time = time.time()\n",
        "\n",
        "# Reading in directory csv as a list\n",
        "with open('2015_present.csv', newline='')as f:\n",
        "    reader = csv.reader(f)\n",
        "    all_races = list(reader)\n",
        "\n",
        "base_url = \"https://www.endurance-data.com\"\n",
        "# Building csvs for each race\n",
        "for race in all_races[2:]:\n",
        "    # Replacing non file directory compatible items\n",
        "    print(race)\n",
        "    race_name = race[0].replace(\" \", \"_\")\n",
        "    race_name = race_name.replace(\".\",\"_\")\n",
        "    race_date = race[1].replace(\"/\",\"_\")\n",
        "\n",
        "    # Skipping race if already recorded\n",
        "    if path.exists('races/'+race_name+race_date+\".csv\"):\n",
        "        continue\n",
        "    # Setting up pagination navigation loop\n",
        "    race_url = base_url + race[-1] + '{}'\n",
        "    main_html = urllib.request.urlopen(race_url.format(1), context=ctx).read()\n",
        "    main_soup = BeautifulSoup(main_html, 'html.parser')\n",
        "    pagination = main_soup(class_='page-item')\n",
        "\n",
        "    # Check if pagination list has at least 2 elements before accessing [-2]\n",
        "    if len(pagination) >= 2:\n",
        "        num_pages = int(pagination[-2].text)\n",
        "    else:\n",
        "        # Handle case where pagination is shorter than expected, e.g., set num_pages to 1\n",
        "        num_pages = 1\n",
        "\n",
        "    ironman_results = []\n",
        "    for i in range(1, num_pages + 1):\n",
        "        html = urllib.request.urlopen(race_url.format(i), context=ctx).read()\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        athletes = soup.find_all(class_='pointer')\n",
        "        # Building athlete data\n",
        "        for athlete in athletes:\n",
        "            athleteStats = []\n",
        "            for stat in athlete:\n",
        "                try:\n",
        "                    athleteStats.append(stat.text)\n",
        "                except AttributeError:\n",
        "                    continue\n",
        "            ironman_results.append(athleteStats)\n",
        "\n",
        "        ironman_dataFrame = pd.DataFrame.from_records(ironman_results)\n",
        "        # Check if the DataFrame is empty before dropping columns\n",
        "        if not ironman_dataFrame.empty:\n",
        "            ironman_dataFrame.drop(ironman_dataFrame.columns[[0, 1]], axis=1, inplace=True)\n",
        "            ironman_dataFrame.rename(columns={2: 'Place',\n",
        "                                              3: 'Name',\n",
        "                                              4: 'Bib',\n",
        "                                              5: 'Division',\n",
        "                                              6: 'Nation',\n",
        "                                              7: 'Swim',\n",
        "                                              8: 'Bike',\n",
        "                                              9: 'Run',\n",
        "                                              10: 'Time'}, inplace=True)\n",
        "            ironman_dataFrame['Race'] = race[0]\n",
        "            ironman_dataFrame['Date'] = race[1]\n",
        "            ironman_dataFrame['Location'] = race[2]\n",
        "        else:\n",
        "            # Handle empty DataFrame, e.g., print a message or skip processing\n",
        "            print(f\"No athletes found for race: {race[0]} on {race[1]}\")\n",
        "\n",
        "    # Saving race dataframe into csv's\n",
        "    # Create the 'races' directory if it doesn't exist\n",
        "    makedirs('races', exist_ok=True) # Create directory if not present\n",
        "\n",
        "    # Check if the DataFrame is not empty before saving it to CSV\n",
        "    if not ironman_dataFrame.empty:\n",
        "        ironman_dataFrame.to_csv(\"races/\"+race_name+race_date+'.csv',index=False)\n",
        "        print(ironman_dataFrame)\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpuzyjYJ333m"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Get the list of files in the 'races' directory\n",
        "races_folder = 'races'\n",
        "if os.path.exists(races_folder):\n",
        "    race_files = os.listdir(races_folder)\n",
        "    print(f\"Files in the '{races_folder}' directory:\")\n",
        "    for file in race_files:\n",
        "        print(file)\n",
        "else:\n",
        "    print(f\"The '{races_folder}' directory does not exist.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
