{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735ebd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('S3_ironman_elevation_updated.csv')\n",
    "print(f\"Loaded {len(df)} races\")\n",
    "print(f\"Number of unique locations: {df['Location'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb8e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nColumns with missing values:\")\n",
    "print(missing_values[missing_values > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061fc5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date_str):\n",
    "    try:\n",
    "        # Parse date in m/d/yyyy format\n",
    "        date_obj = datetime.datetime.strptime(date_str, '%m/%d/%Y')\n",
    "        # Return in yyyy-mm-dd format\n",
    "        return date_obj.strftime('%Y-%m-%d')\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing date {date_str}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67fcc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['api_date'] = df['Date'].apply(parse_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41335df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Date', 'api_date']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa091a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_weather(latitude, longitude, date):\n",
    "    \"\"\"Fetch historical weather data from Open-Meteo API\n",
    "    \n",
    "    Args:\n",
    "        latitude (float): Location latitude\n",
    "        longitude (float): Location longitude\n",
    "        date (str): Date in yyyy-mm-dd format\n",
    "        \n",
    "    Returns:\n",
    "        dict: Weather data or None if request failed\n",
    "    \"\"\"\n",
    "    # Base URL for historical data\n",
    "    base_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    \n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"start_date\": date,\n",
    "        \"end_date\": date,\n",
    "        \"daily\": [\"temperature_2m_max\", \"temperature_2m_min\", \"precipitation_sum\", \"rain_sum\",\n",
    "                 \"weathercode\", \"windspeed_10m_max\", \"windgusts_10m_max\", \"winddirection_10m_dominant\"],\n",
    "        \"hourly\": [\"temperature_2m\", \"relativehumidity_2m\", \"pressure_msl\", \n",
    "                  \"cloudcover\", \"windspeed_10m\", \"direct_radiation\", \"diffuse_radiation\"],\n",
    "        \"timezone\": \"auto\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"API request failed with status code {response.status_code}: {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error making API request: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def process_weather_data(api_response):\n",
    "    \"\"\"Extract relevant weather metrics from Open-Meteo API response\n",
    "    \n",
    "    Args:\n",
    "        api_response (dict): Response from Open-Meteo API\n",
    "        \n",
    "    Returns:\n",
    "        dict: Extracted weather metrics\n",
    "    \"\"\"\n",
    "    if not api_response or 'hourly' not in api_response or 'daily' not in api_response:\n",
    "        return {}\n",
    "    \n",
    "    weather_data = {}\n",
    "    \n",
    "    # Daily metrics\n",
    "    daily = api_response['daily']\n",
    "    weather_data['max_temperature'] = daily['temperature_2m_max'][0]  # Max temperature (°C)\n",
    "    weather_data['min_temperature'] = daily['temperature_2m_min'][0]  # Min temperature (°C)\n",
    "    \n",
    "    # Hourly metrics - get 10AM data (hour 10)\n",
    "    hourly = api_response['hourly']\n",
    "    # Find the index for 10:00 AM\n",
    "    hour_10_index = None\n",
    "    for i, time in enumerate(hourly['time']):\n",
    "        if time.endswith('T10:00'):\n",
    "            hour_10_index = i\n",
    "            break\n",
    "    \n",
    "    if hour_10_index is not None:\n",
    "        weather_data['temperature_10AM'] = hourly['temperature_2m'][hour_10_index]  # Temp at 10 AM (°C)\n",
    "        weather_data['relative_humidity'] = hourly['relativehumidity_2m'][hour_10_index]  # RH at 10 AM (%)\n",
    "        weather_data['cloud_coverage'] = hourly['cloudcover'][hour_10_index]  # Cloud cover at 10 AM (%)\n",
    "        \n",
    "    # Calculate daily averages\n",
    "    weather_data['average_wind_speed'] = sum(hourly['windspeed_10m']) / len(hourly['windspeed_10m'])  # Avg wind speed (km/h)\n",
    "    weather_data['average_pressure'] = sum(hourly['pressure_msl']) / len(hourly['pressure_msl'])  # Avg pressure (hPa)\n",
    "    \n",
    "    # Calculate solar radiation (average of direct + diffuse during daylight hours)\n",
    "    if 'direct_radiation' in hourly and 'diffuse_radiation' in hourly:\n",
    "        # Only consider daylight hours (where radiation > 0)\n",
    "        direct = np.array(hourly['direct_radiation'])\n",
    "        diffuse = np.array(hourly['diffuse_radiation'])\n",
    "        total_radiation = direct + diffuse\n",
    "        daylight_radiation = total_radiation[total_radiation > 0]\n",
    "        if len(daylight_radiation) > 0:\n",
    "            weather_data['solar_radiation'] = daylight_radiation.mean()  # Average solar radiation (W/m²)\n",
    "    \n",
    "    return weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d3baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_row = df.iloc[0]\n",
    "print(f\"Testing with: {test_row['Location']} on {test_row['Date']} (API Date: {test_row['api_date']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9381078",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response = get_historical_weather(test_row['latitude'], test_row['longitude'], test_row['api_date'])\n",
    "\n",
    "if api_response:\n",
    "    print(\"API request successful!\")\n",
    "    weather_data = process_weather_data(api_response)\n",
    "    print(\"\\nExtracted weather data:\")\n",
    "    for key, value in weather_data.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"API request failed - please check your internet connection or API availability\")\n",
    "\n",
    "\n",
    "# Fetch weather data for each race with rate limiting to avoid API blocks\n",
    "def fetch_all_weather_data(df, max_rows=None):\n",
    "    \"\"\"Fetch weather data for races in the dataframe\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe with race information\n",
    "        max_rows (int, optional): Maximum number of rows to process. If None, process all rows.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Updated dataframe with weather data\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Keep track of races processed\n",
    "    processed = 0\n",
    "    success_count = 0\n",
    "    \n",
    "    # Determine how many rows to process\n",
    "    rows_to_process = len(result_df) if max_rows is None else min(max_rows, len(result_df))\n",
    "    subset_df = result_df.iloc[:rows_to_process]\n",
    "    \n",
    "    # Loop through each race\n",
    "    for idx in tqdm(subset_df.index, desc=\"Fetching weather data\"):\n",
    "        # Get location and date information\n",
    "        lat = result_df.at[idx, 'latitude']\n",
    "        lon = result_df.at[idx, 'longitude']\n",
    "        date = result_df.at[idx, 'api_date']\n",
    "        location = result_df.at[idx, 'Location']\n",
    "        \n",
    "        # Skip if date is not valid\n",
    "        if pd.isna(date) or not date:\n",
    "            print(f\"Skipping row {idx} due to invalid date\")\n",
    "            continue\n",
    "            \n",
    "        # Make API request\n",
    "        api_response = get_historical_weather(lat, lon, date)\n",
    "        \n",
    "        # Process response and update dataframe\n",
    "        if api_response:\n",
    "            weather_data = process_weather_data(api_response)\n",
    "            \n",
    "            # Update dataframe with weather data\n",
    "            for col, value in weather_data.items():\n",
    "                if col in result_df.columns:\n",
    "                    result_df.at[idx, col] = value\n",
    "            \n",
    "            success_count += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for {location} on {date}\")\n",
    "        \n",
    "        # Increment counter\n",
    "        processed += 1\n",
    "        \n",
    "        # Add a delay every 5 requests to avoid rate limiting\n",
    "        if processed % 5 == 0:\n",
    "            time.sleep(1)  # Sleep for 1 second\n",
    "    \n",
    "    print(f\"Successfully retrieved data for {success_count} out of {processed} races.\")\n",
    "    return result_df\n",
    "\n",
    "# First process a small sample to test (10 races)\n",
    "print(\"Processing sample data (first 10 races)...\")\n",
    "sample_size = 10\n",
    "updated_sample_df = fetch_all_weather_data(df, max_rows=sample_size)\n",
    "\n",
    "# Show the results of the sample\n",
    "print(\"\\nSample results:\")\n",
    "updated_sample_df[['Location', 'Date', 'max_temperature', 'min_temperature', \n",
    "                   'relative_humidity', 'average_wind_speed']].head(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cf17bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_full = input(\"Process the full dataset? (yes/no): \")\n",
    "process_full = 'yes'\n",
    "if process_full.lower() == 'yes':\n",
    "    print(\"\\nProcessing the full dataset (this may take some time)...\")\n",
    "    updated_df = fetch_all_weather_data(df)\n",
    "    \n",
    "    # Display summary of filled values\n",
    "    filled_count = {}\n",
    "    for col in ['max_temperature', 'min_temperature', 'temperature_10AM', 'relative_humidity', \n",
    "                'average_wind_speed', 'average_pressure', 'cloud_coverage', 'solar_radiation']:\n",
    "        filled_count[col] = updated_df[col].notnull().sum()\n",
    "    \n",
    "    print(\"\\nNumber of filled values per column:\")\n",
    "    for col, count in filled_count.items():\n",
    "        print(f\"{col}: {count} out of {len(updated_df)} ({count/len(updated_df)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"Full dataset processing skipped.\")\n",
    "    # Make a copy so the next cells still work\n",
    "    updated_df = updated_sample_df.copy()\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "output_filename = 'S3_ironman_elevation_weather_updated.csv'\n",
    "updated_df.to_csv(output_filename, index=False)\n",
    "print(f\"Updated data saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad3c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "## Data Visualization\n",
    "updated_df = pd.read_csv('S3_ironman_elevation_weather_updated.csv')\n",
    "# Create some visualizations to explore the retrieved weather data\n",
    "if updated_df['max_temperature'].notnull().sum() > 0:\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    # Plot 1: Distribution of maximum temperatures\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.histplot(updated_df['max_temperature'].dropna(), kde=True)\n",
    "    plt.title('Distribution of Maximum Temperatures')\n",
    "    plt.xlabel('Temperature (°C)')\n",
    "    \n",
    "    # Plot 2: Average wind speed vs cloud coverage\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.scatterplot(data=updated_df.dropna(subset=['average_wind_speed', 'cloud_coverage']), \n",
    "                   x='average_wind_speed', y='cloud_coverage')\n",
    "    plt.title('Wind Speed vs Cloud Coverage')\n",
    "    plt.xlabel('Average Wind Speed (km/h)')\n",
    "    plt.ylabel('Cloud Coverage (%)')\n",
    "    \n",
    "    # Plot 3: Temperature variation (max - min) by location (top 10 locations)\n",
    "    plt.subplot(2, 2, 3)\n",
    "    temp_variation = updated_df.dropna(subset=['max_temperature', 'min_temperature']).copy()\n",
    "    temp_variation['temp_range'] = temp_variation['max_temperature'] - temp_variation['min_temperature']\n",
    "    top_locations = temp_variation.groupby('Location')['temp_range'].mean().nlargest(10)\n",
    "    \n",
    "    sns.barplot(x=top_locations.values, y=top_locations.index)\n",
    "    plt.title('Top 10 Locations by Temperature Range')\n",
    "    plt.xlabel('Average Temperature Range (°C)')\n",
    "    \n",
    "    # Plot 4: Correlation between max temperature and relative humidity\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.regplot(data=updated_df.dropna(subset=['max_temperature', 'relative_humidity']), \n",
    "               x='max_temperature', y='relative_humidity')\n",
    "    plt.title('Temperature vs Relative Humidity')\n",
    "    plt.xlabel('Maximum Temperature (°C)')\n",
    "    plt.ylabel('Relative Humidity (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No weather data available for visualization\")\n",
    "\n",
    "# Create a correlation heatmap for numerical columns\n",
    "weather_cols = ['max_temperature', 'min_temperature', 'temperature_10AM', \n",
    "               'relative_humidity', 'average_wind_speed', 'average_pressure', \n",
    "               'cloud_coverage', 'solar_radiation']\n",
    "\n",
    "# Filter columns that have data\n",
    "available_cols = [col for col in weather_cols if updated_df[col].notnull().sum() > 0]\n",
    "\n",
    "if len(available_cols) > 1:  # Need at least 2 columns for correlation\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = updated_df[available_cols].corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', \n",
    "               linewidths=0.5, mask=mask)\n",
    "    plt.title('Correlation Between Weather Variables')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough data columns for correlation analysis\")\n",
    "\n",
    "## Analysis of Temperature by Race Location\n",
    "\n",
    "\n",
    "# Analyze temperature by location\n",
    "if updated_df['max_temperature'].notnull().sum() > 0:\n",
    "    # Group by location and calculate average temperatures\n",
    "    location_temps = updated_df.groupby('Location').agg({\n",
    "        'max_temperature': 'mean',\n",
    "        'min_temperature': 'mean',\n",
    "        'temperature_10AM': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Sort by max temperature\n",
    "    location_temps_sorted = location_temps.sort_values('max_temperature', ascending=False)\n",
    "    \n",
    "    # Plot top 15 hottest and coldest locations\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Top 15 hottest\n",
    "    plt.subplot(2, 1, 1)\n",
    "    top_hot = location_temps_sorted.head(15)\n",
    "    \n",
    "    x = np.arange(len(top_hot))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x - width, top_hot['max_temperature'], width, label='Max')\n",
    "    plt.bar(x, top_hot['temperature_10AM'], width, label='10 AM')\n",
    "    plt.bar(x + width, top_hot['min_temperature'], width, label='Min')\n",
    "    \n",
    "    plt.xlabel('Location')\n",
    "    plt.ylabel('Temperature (°C)')\n",
    "    plt.title('Top 15 Locations with Highest Temperatures')\n",
    "    plt.xticks(x, top_hot['Location'], rotation=90)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Top 15 coldest\n",
    "    plt.subplot(2, 1, 2)\n",
    "    top_cold = location_temps_sorted.tail(15).iloc[::-1]  # Reverse to show coldest first\n",
    "    \n",
    "    x = np.arange(len(top_cold))\n",
    "    \n",
    "    plt.bar(x - width, top_cold['max_temperature'], width, label='Max')\n",
    "    plt.bar(x, top_cold['temperature_10AM'], width, label='10 AM')\n",
    "    plt.bar(x + width, top_cold['min_temperature'], width, label='Min')\n",
    "    \n",
    "    plt.xlabel('Location')\n",
    "    plt.ylabel('Temperature (°C)')\n",
    "    plt.title('Top 15 Locations with Lowest Temperatures')\n",
    "    plt.xticks(x, top_cold['Location'], rotation=90)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No temperature data available for analysis\")\n",
    "\n",
    "# Additional analysis: Weather conditions across different years\n",
    "if updated_df['max_temperature'].notnull().sum() > 0:\n",
    "    # Extract year from api_date\n",
    "    updated_df['year'] = updated_df['api_date'].str[:4].astype(int)\n",
    "    \n",
    "    # Group by year and calculate average values\n",
    "    yearly_weather = updated_df.groupby('year').agg({\n",
    "        'max_temperature': 'mean',\n",
    "        'min_temperature': 'mean',\n",
    "        'average_wind_speed': 'mean',\n",
    "        'relative_humidity': 'mean',\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "    \n",
    "    # Temperature plot\n",
    "    ax1.plot(yearly_weather['year'], yearly_weather['max_temperature'], 'ro-', label='Max Temp')\n",
    "    ax1.plot(yearly_weather['year'], yearly_weather['min_temperature'], 'bo-', label='Min Temp')\n",
    "    ax1.set_ylabel('Temperature (°C)')\n",
    "    ax1.set_title('Average Temperatures Over Years')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Wind and humidity plot\n",
    "    ax2.plot(yearly_weather['year'], yearly_weather['average_wind_speed'], 'go-', label='Wind Speed')\n",
    "    ax2.set_ylabel('Wind Speed (km/h)', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "    ax2.set_xlabel('Year')\n",
    "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add second y-axis for humidity\n",
    "    ax3 = ax2.twinx()\n",
    "    ax3.plot(yearly_weather['year'], yearly_weather['relative_humidity'], 'co-', label='Humidity')\n",
    "    ax3.set_ylabel('Relative Humidity (%)', color='c')\n",
    "    ax3.tick_params(axis='y', labelcolor='c')\n",
    "    \n",
    "    # Add combined legend\n",
    "    lines1, labels1 = ax2.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax3.get_legend_handles_labels()\n",
    "    ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough data for yearly analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
