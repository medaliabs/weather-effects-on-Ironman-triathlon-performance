{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7707fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Improved Ironman Elevation Data Scraper\n",
    "\n",
    "This script extracts bike and run elevation data from multiple sources:\n",
    "1. TriathlonCourseInfo.com (primary source)\n",
    "2. PJammCycling.com (specialized cycling data)\n",
    "3. Ironman.com official website (authoritative data)\n",
    "\n",
    "It combines data from these sources with a priority system to ensure accuracy.\n",
    "'''\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from urllib.parse import quote, urljoin\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"elevation_scraper.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def clean_race_name(race):\n",
    "    \"\"\"\n",
    "    Cleans a race name for better search results.\n",
    "    Removes parentheses, handles special cases, and standardizes names.\n",
    "    \"\"\"\n",
    "    # Remove any text in parentheses and trailing whitespace\n",
    "    cleaned = re.sub(r'\\s*\\([^)]*\\)', '', race).strip().lower()\n",
    "    \n",
    "    # Handle special cases (extend as needed)\n",
    "    special_cases = {\n",
    "        'hawaii': 'kona',\n",
    "        'wc hawaii': 'kona',\n",
    "        'lake placid': 'lake placid',\n",
    "        'mont-tremblant': 'mont tremblant',\n",
    "        'whistler': 'whistler',\n",
    "        'wisconsin': 'wisconsin',\n",
    "        'florida': 'florida',\n",
    "        'texas': 'texas',\n",
    "        'arizona': 'arizona',\n",
    "        'chattanooga': 'chattanooga',\n",
    "        'coeur': 'coeur d\\'alene',\n",
    "        'maryland': 'maryland',\n",
    "        'louisville': 'louisville',\n",
    "        'california': 'california'\n",
    "    }\n",
    "    \n",
    "    for key, value in special_cases.items():\n",
    "        if key in cleaned:\n",
    "            return value\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def clean_location_name(location):\n",
    "    \"\"\"\n",
    "    Cleans a location name for search, focusing on the city name.\n",
    "    \"\"\"\n",
    "    # Remove parentheses and trailing whitespace\n",
    "    location = re.sub(r'\\s*\\([^)]*\\)', '', location).strip()\n",
    "    \n",
    "    # Split by comma and take first part (city name)\n",
    "    parts = location.split(',')\n",
    "    if len(parts) > 0:\n",
    "        city = parts[0].strip().lower()\n",
    "        \n",
    "        # Handle special cases\n",
    "        special_cases = {\n",
    "            \"whistler/pemberton\": \"whistler\",\n",
    "            \"the woodlands\": \"the woodlands texas\",\n",
    "            \"mont-tremblant\": \"mont tremblant\",\n",
    "            \"alc√∫dia\": \"mallorca\",\n",
    "            \"alcudia\": \"mallorca\",\n",
    "            \"mar del plata\": \"mar del plata argentina\",\n",
    "            \"lake placid\": \"lake placid\",\n",
    "            \"kona\": \"kona hawaii\",\n",
    "            \"santa rosa\": \"santa rosa california\",\n",
    "            \"panama city beach\": \"panama city beach florida\",\n",
    "            \"coeur d'alene\": \"coeur d'alene idaho\"\n",
    "        }\n",
    "        \n",
    "        for key, value in special_cases.items():\n",
    "            if key in city:\n",
    "                return value\n",
    "        \n",
    "        return city\n",
    "    \n",
    "    return location.lower()\n",
    "\n",
    "def extract_number(text):\n",
    "    \"\"\"\n",
    "    Extracts numeric values from text, accounting for various formats.\n",
    "    Returns the largest number found, or None if no numbers are found.\n",
    "    \"\"\"\n",
    "    # Handle both comma-separated and non-comma formats (e.g., 1,500 and 1500)\n",
    "    matches = re.findall(r'(\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\d+(?:\\.\\d+)?)', text)\n",
    "    \n",
    "    if matches:\n",
    "        # Convert matches to numbers, removing commas\n",
    "        nums = [float(m.replace(',', '')) for m in matches]\n",
    "        # Return the largest as an integer (most likely the total elevation)\n",
    "        return int(max(nums))\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_units(text):\n",
    "    \"\"\"\n",
    "    Identifies whether the elevation is in feet or meters.\n",
    "    Returns 'ft' or 'm' or None if undetermined.\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    if 'feet' in text_lower or 'ft' in text_lower or \"'\" in text_lower:\n",
    "        return 'ft'\n",
    "    elif 'meter' in text_lower or 'm' in text_lower:\n",
    "        return 'm'\n",
    "    return None\n",
    "\n",
    "def convert_to_feet(value, unit):\n",
    "    \"\"\"\n",
    "    Converts elevation values to feet if they're in meters.\n",
    "    \"\"\"\n",
    "    if unit == 'm':  # Convert meters to feet\n",
    "        return int(value * 3.28084)\n",
    "    return value  # Already in feet\n",
    "\n",
    "def scrape_triathloncourseinfo(url):\n",
    "    \"\"\"\n",
    "    Scrapes elevation data from TriathlonCourseInfo websites.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Scraping TriathlonCourseInfo: {url}\")\n",
    "        response = requests.get(url, headers=headers, timeout=20)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            logger.warning(f\"Failed to access {url}. Status code: {response.status_code}\")\n",
    "            return None, None\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        bike_elev = None\n",
    "        run_elev = None\n",
    "        bike_unit = None\n",
    "        run_unit = None\n",
    "        \n",
    "        # Look first inside any div.course-stats if present\n",
    "        stats_section = soup.find(\"div\", {\"class\": \"course-stats\"})\n",
    "        if stats_section:\n",
    "            candidates = stats_section.find_all([\"div\", \"p\", \"li\", \"h3\", \"h4\"])\n",
    "        else:\n",
    "            # Fallback: look through all relevant tags\n",
    "            candidates = soup.find_all([\"p\", \"div\", \"li\", \"h3\", \"h4\", \"span\"])\n",
    "            \n",
    "        # Additional method: look for sections with specific headings\n",
    "        bike_section = None\n",
    "        run_section = None\n",
    "        \n",
    "        h_tags = soup.find_all(['h2', 'h3', 'h4', 'h5'])\n",
    "        for h in h_tags:\n",
    "            text = h.get_text().lower()\n",
    "            if 'bike' in text or 'cycling' in text:\n",
    "                bike_section = h\n",
    "            elif 'run' in text or 'marathon' in text:\n",
    "                run_section = h\n",
    "                \n",
    "        # Process sections if found\n",
    "        if bike_section:\n",
    "            siblings = []\n",
    "            for sibling in bike_section.next_siblings:\n",
    "                if sibling.name in ['h2', 'h3', 'h4', 'h5']:\n",
    "                    break\n",
    "                siblings.append(sibling)\n",
    "            \n",
    "            for sibling in siblings:\n",
    "                if hasattr(sibling, 'get_text'):\n",
    "                    text = sibling.get_text().lower()\n",
    "                    if 'elevation' in text or 'gain' in text or 'climbing' in text:\n",
    "                        bike_elev = extract_number(text)\n",
    "                        bike_unit = extract_units(text)\n",
    "                        break\n",
    "        \n",
    "        if run_section:\n",
    "            siblings = []\n",
    "            for sibling in run_section.next_siblings:\n",
    "                if sibling.name in ['h2', 'h3', 'h4', 'h5']:\n",
    "                    break\n",
    "                siblings.append(sibling)\n",
    "                \n",
    "            for sibling in siblings:\n",
    "                if hasattr(sibling, 'get_text'):\n",
    "                    text = sibling.get_text().lower()\n",
    "                    if 'elevation' in text or 'gain' in text or 'climbing' in text:\n",
    "                        run_elev = extract_number(text)\n",
    "                        run_unit = extract_units(text)\n",
    "                        break\n",
    "                        \n",
    "        # Process all candidate elements\n",
    "        for elem in candidates:\n",
    "            text = elem.get_text(separator=\" \").lower()\n",
    "            \n",
    "            # Extract bike elevation data\n",
    "            bike_keywords = ['bike elevation', 'bike climbing', 'cycling elevation', 'bike course elevation',\n",
    "                            'bike elevation gain', 'bike course climbing', 'bike vertical']\n",
    "            if any(keyword in text for keyword in bike_keywords) and 'run' not in text:\n",
    "                val = extract_number(text)\n",
    "                unit = extract_units(text)\n",
    "                if val and not bike_elev:\n",
    "                    bike_elev = val\n",
    "                    bike_unit = unit\n",
    "                    \n",
    "            # Extract run elevation data\n",
    "            run_keywords = ['run elevation', 'run climbing', 'running elevation', 'run course elevation',\n",
    "                           'run elevation gain', 'run course climbing', 'run vertical', 'marathon elevation']\n",
    "            if any(keyword in text for keyword in run_keywords) and 'bike' not in text:\n",
    "                val = extract_number(text)\n",
    "                unit = extract_units(text)\n",
    "                if val and not run_elev:\n",
    "                    run_elev = val\n",
    "                    run_unit = unit\n",
    "                    \n",
    "        # Convert to feet if necessary\n",
    "        if bike_elev and bike_unit:\n",
    "            bike_elev = convert_to_feet(bike_elev, bike_unit)\n",
    "        if run_elev and run_unit:\n",
    "            run_elev = convert_to_feet(run_elev, run_unit)\n",
    "            \n",
    "        logger.info(f\"TriathlonCourseInfo data: Bike={bike_elev}ft, Run={run_elev}ft\")\n",
    "        return bike_elev, run_elev\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error scraping {url}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def scrape_pjammcycling(race_name):\n",
    "    \"\"\"\n",
    "    Scrapes elevation data from PJammCycling.com.\n",
    "    This site specializes in detailed bike course information.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # Attempt to find the appropriate page\n",
    "    base_url = \"https://pjammcycling.com/triathlon/\"\n",
    "    search_terms = clean_race_name(race_name).split()\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Attempting to scrape PJammCycling for {race_name}\")\n",
    "        \n",
    "        # First approach: Try to get the search results page\n",
    "        response = requests.get(\"https://pjammcycling.com/search.php\", \n",
    "                                params={\"search\": f\"IRONMAN {race_name}\"}, \n",
    "                                headers=headers, \n",
    "                                timeout=20)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            links = soup.find_all('a', href=True)\n",
    "            \n",
    "            triathlon_links = [link for link in links if '/triathlon/' in link['href']]\n",
    "            \n",
    "            if triathlon_links:\n",
    "                # Find the most relevant link\n",
    "                most_relevant = None\n",
    "                for link in triathlon_links:\n",
    "                    link_text = link.get_text().lower()\n",
    "                    if race_name.lower() in link_text and \"ironman\" in link_text:\n",
    "                        most_relevant = link['href']\n",
    "                        break\n",
    "                \n",
    "                if most_relevant:\n",
    "                    # Use the found link\n",
    "                    url = urljoin(\"https://pjammcycling.com/\", most_relevant)\n",
    "                    detail_response = requests.get(url, headers=headers, timeout=20)\n",
    "                    \n",
    "                    if detail_response.status_code == 200:\n",
    "                        detail_soup = BeautifulSoup(detail_response.content, 'html.parser')\n",
    "                        \n",
    "                        # Find the elevation information\n",
    "                        bike_elev = None\n",
    "                        run_elev = None\n",
    "                        \n",
    "                        # PJammCycling typically has structured data in tables or specific formats\n",
    "                        stats_divs = detail_soup.find_all('div', class_='stat')\n",
    "                        \n",
    "                        for stat in stats_divs:\n",
    "                            text = stat.get_text().lower()\n",
    "                            \n",
    "                            if 'elevation gain' in text:\n",
    "                                # Extract the bike or run elevation\n",
    "                                if 'bike' in text:\n",
    "                                    bike_elev = extract_number(text)\n",
    "                                elif 'run' in text:\n",
    "                                    run_elev = extract_number(text)\n",
    "                        \n",
    "                        # Alternative way: look for course summary sections\n",
    "                        summary_texts = [p.get_text().lower() for p in detail_soup.find_all('p')]\n",
    "                        for text in summary_texts:\n",
    "                            # Look for bike elevation data\n",
    "                            if 'bike' in text and ('elevation' in text or 'climbing' in text):\n",
    "                                val = extract_number(text)\n",
    "                                if val and not bike_elev:\n",
    "                                    bike_elev = val\n",
    "                                    \n",
    "                            # Look for run elevation data\n",
    "                            if 'run' in text and ('elevation' in text or 'climbing' in text):\n",
    "                                val = extract_number(text)\n",
    "                                if val and not run_elev:\n",
    "                                    run_elev = val\n",
    "                        \n",
    "                        logger.info(f\"PJammCycling data: Bike={bike_elev}ft, Run={run_elev}ft\")\n",
    "                        return bike_elev, run_elev\n",
    "        \n",
    "        logger.warning(\"PJammCycling - No relevant data found\")\n",
    "        return None, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error scraping PJammCycling for {race_name}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def scrape_ironman_official(race_name):\n",
    "    \"\"\"\n",
    "    Scrapes elevation data from the official Ironman website.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # Attempt to create a URL for the race on ironman.com\n",
    "    clean_name = clean_race_name(race_name).replace(\" \", \"-\")\n",
    "    \n",
    "    # Try different URL patterns that Ironman.com might use\n",
    "    url_patterns = [\n",
    "        f\"https://www.ironman.com/im-{clean_name}-course\",\n",
    "        f\"https://www.ironman.com/im-{clean_name}\",\n",
    "        f\"https://www.ironman.com/ironman-{clean_name}-course\",\n",
    "        f\"https://www.ironman.com/ironman-{clean_name}\"\n",
    "    ]\n",
    "    \n",
    "    bike_elev = None\n",
    "    run_elev = None\n",
    "    \n",
    "    for url in url_patterns:\n",
    "        try:\n",
    "            logger.info(f\"Trying Ironman.com URL: {url}\")\n",
    "            response = requests.get(url, headers=headers, timeout=20)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                logger.info(f\"Successfully accessed Ironman.com at {url}\")\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Look for course sections\n",
    "                course_section = None\n",
    "                for section in soup.find_all('section'):\n",
    "                    if section.get('id') == 'course' or 'course' in section.get('class', []):\n",
    "                        course_section = section\n",
    "                        break\n",
    "                \n",
    "                # If we found a dedicated course section, focus on it\n",
    "                if course_section:\n",
    "                    soup = course_section\n",
    "                \n",
    "                # Look for bike course information\n",
    "                bike_section = None\n",
    "                run_section = None\n",
    "                \n",
    "                # Find sections by headings\n",
    "                for heading in soup.find_all(['h2', 'h3', 'h4']):\n",
    "                    text = heading.get_text().lower()\n",
    "                    if 'bike' in text and not bike_section:\n",
    "                        bike_section = heading\n",
    "                    elif 'run' in text and not run_section:\n",
    "                        run_section = heading\n",
    "                \n",
    "                # Process bike section\n",
    "                if bike_section:\n",
    "                    next_elements = []\n",
    "                    current = bike_section.next_sibling\n",
    "                    \n",
    "                    # Collect next elements until we hit another heading or section\n",
    "                    while current and current.name not in ['h2', 'h3', 'h4', 'section']:\n",
    "                        if hasattr(current, 'name') and current.name:\n",
    "                            next_elements.append(current)\n",
    "                        current = current.next_sibling\n",
    "                    \n",
    "                    for elem in next_elements:\n",
    "                        text = elem.get_text().lower()\n",
    "                        if 'elevation' in text or 'gain' in text or 'climbing' in text or 'ascent' in text:\n",
    "                            bike_elev = extract_number(text)\n",
    "                            unit = extract_units(text)\n",
    "                            if unit:\n",
    "                                bike_elev = convert_to_feet(bike_elev, unit)\n",
    "                            break\n",
    "                \n",
    "                # Process run section\n",
    "                if run_section:\n",
    "                    next_elements = []\n",
    "                    current = run_section.next_sibling\n",
    "                    \n",
    "                    # Collect next elements until we hit another heading or section\n",
    "                    while current and current.name not in ['h2', 'h3', 'h4', 'section']:\n",
    "                        if hasattr(current, 'name') and current.name:\n",
    "                            next_elements.append(current)\n",
    "                        current = current.next_sibling\n",
    "                    \n",
    "                    for elem in next_elements:\n",
    "                        text = elem.get_text().lower()\n",
    "                        if 'elevation' in text or 'gain' in text or 'climbing' in text or 'ascent' in text:\n",
    "                            run_elev = extract_number(text)\n",
    "                            unit = extract_units(text)\n",
    "                            if unit:\n",
    "                                run_elev = convert_to_feet(run_elev, unit)\n",
    "                            break\n",
    "                \n",
    "                # If we found at least one value, we can stop trying URLs\n",
    "                if bike_elev or run_elev:\n",
    "                    logger.info(f\"Ironman.com data: Bike={bike_elev}ft, Run={run_elev}ft\")\n",
    "                    return bike_elev, run_elev\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error accessing {url}: {str(e)}\")\n",
    "    \n",
    "    logger.warning(\"No data found from Ironman.com\")\n",
    "    return None, None\n",
    "\n",
    "def search_triathloncourseinfo(race_name, location=None):\n",
    "    \"\"\"\n",
    "    Searches TriathlonCourseInfo.com for a specific race and returns elevation data.\n",
    "    Uses both race name and location for better search results.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # Try different search strategies\n",
    "    search_queries = []\n",
    "    \n",
    "    # Option 1: Clean race name\n",
    "    clean_race = clean_race_name(race_name)\n",
    "    search_queries.append(f\"ironman {clean_race}\")\n",
    "    \n",
    "    # Option 2: Add location if available\n",
    "    if location:\n",
    "        clean_loc = clean_location_name(location)\n",
    "        search_queries.append(f\"ironman {clean_race} {clean_loc}\")\n",
    "        search_queries.append(f\"ironman {clean_loc}\")\n",
    "    \n",
    "    base_search = \"https://triathloncourseinfo.com/?s=\"\n",
    "    \n",
    "    for query in search_queries:\n",
    "        try:\n",
    "            logger.info(f\"Searching TriathlonCourseInfo for: {query}\")\n",
    "            search_url = base_search + quote(query)\n",
    "            \n",
    "            response = requests.get(search_url, headers=headers, timeout=20)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Find all article results\n",
    "                articles = soup.find_all(\"article\")\n",
    "                candidate_urls = []\n",
    "                \n",
    "                for art in articles:\n",
    "                    h2 = art.find(\"h2\", {\"class\": \"entry-title\"})\n",
    "                    if not h2:\n",
    "                        continue\n",
    "                    \n",
    "                    link = h2.find(\"a\")\n",
    "                    if not link or not link.get(\"href\"):\n",
    "                        continue\n",
    "                    \n",
    "                    title_text = h2.get_text().lower()\n",
    "                    \n",
    "                    # Only consider Ironman events and exclude 70.3 (half Ironman) if searching for full\n",
    "                    ironman_keywords = [\"ironman\", \"im \", \"140.6\"]\n",
    "                    if any(kw in title_text for kw in ironman_keywords) and (\"70.3\" not in title_text or \"70.3\" in race_name.lower()):\n",
    "                        relevance_score = 0\n",
    "                        \n",
    "                        # Higher score if race name is in title\n",
    "                        if clean_race in title_text:\n",
    "                            relevance_score += 3\n",
    "                            \n",
    "                        # Higher score if location is in title\n",
    "                        if location and clean_location_name(location) in title_text:\n",
    "                            relevance_score += 2\n",
    "                            \n",
    "                        # Add race name keywords for additional matching\n",
    "                        for word in clean_race.split():\n",
    "                            if word in title_text and word not in [\"ironman\", \"im\"]:\n",
    "                                relevance_score += 1\n",
    "                        \n",
    "                        candidate_urls.append((link[\"href\"], relevance_score))\n",
    "                \n",
    "                # Sort by relevance score (highest first)\n",
    "                candidate_urls.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                # Check up to 3 most relevant pages\n",
    "                for url, _ in candidate_urls[:3]:\n",
    "                    logger.info(f\"Checking relevant page: {url}\")\n",
    "                    bike, run = scrape_triathloncourseinfo(url)\n",
    "                    \n",
    "                    if bike is not None or run is not None:\n",
    "                        return bike, run\n",
    "                        \n",
    "                # If we found candidates but no elevation data, try the next search query\n",
    "                if candidate_urls:\n",
    "                    logger.info(f\"Found candidate pages but no elevation data for: {query}\")\n",
    "                    continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error searching TriathlonCourseInfo for {query}: {str(e)}\")\n",
    "    \n",
    "    # If we get here, we didn't find anything\n",
    "    return None, None\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process the CSV and extract elevation data.\n",
    "    \"\"\"\n",
    "    # Load the CSV file\n",
    "    try:\n",
    "        csv_path = \"S3_70.3_locations.csv_coord_elevation.csv\"\n",
    "        logger.info(f\"Loading CSV file: {csv_path}\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading CSV: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # Check if elevation columns exist, create them if not\n",
    "    if \"bike elevation\" not in df.columns:\n",
    "        df[\"bike elevation\"] = None\n",
    "    if \"run elevation\" not in df.columns:\n",
    "        df[\"run elevation\"] = None\n",
    "    \n",
    "    # Create a dictionary to cache results by race name\n",
    "    race_data_cache = {}\n",
    "    \n",
    "    # Count total races that need processing\n",
    "    unique_races = set()\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[\"Race\"]) or str(row[\"Race\"]).strip() == \"\":\n",
    "            continue\n",
    "        unique_races.add(row[\"Race\"])\n",
    "    \n",
    "    total_races = len(unique_races)\n",
    "    processed = 0\n",
    "    \n",
    "    # Process each unique race\n",
    "    for race_name in unique_races:\n",
    "        processed += 1\n",
    "        logger.info(f\"Processing race {processed}/{total_races}: {race_name}\")\n",
    "        \n",
    "        # Skip if already cached\n",
    "        if race_name in race_data_cache:\n",
    "            logger.info(f\"Using cached data for {race_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Get the first location for this race (used for searching)\n",
    "        location = None\n",
    "        for _, row in df.iterrows():\n",
    "            if row[\"Race\"] == race_name and pd.notna(row[\"Location\"]):\n",
    "                location = row[\"Location\"]\n",
    "                break\n",
    "        \n",
    "        bike_elevation = None\n",
    "        run_elevation = None\n",
    "        \n",
    "        # Strategy 1: Try TriathlonCourseInfo search\n",
    "        logger.info(f\"Strategy 1: Searching TriathlonCourseInfo for {race_name}\")\n",
    "        bike_elevation, run_elevation = search_triathloncourseinfo(race_name, location)\n",
    "        \n",
    "        # Strategy 2: If not found, try PJammCycling\n",
    "        if not bike_elevation or not run_elevation:\n",
    "            logger.info(f\"Strategy 2: Searching PJammCycling for {race_name}\")\n",
    "            bike_pjamm, run_pjamm = scrape_pjammcycling(race_name)\n",
    "            \n",
    "            # Use PJammCycling data if available\n",
    "            if bike_pjamm and not bike_elevation:\n",
    "                bike_elevation = bike_pjamm\n",
    "            if run_pjamm and not run_elevation:\n",
    "                run_elevation = run_pjamm\n",
    "        \n",
    "        # Strategy 3: As a last resort, check Ironman's official site\n",
    "        if not bike_elevation or not run_elevation:\n",
    "            logger.info(f\"Strategy 3: Checking Ironman.com for {race_name}\")\n",
    "            bike_im, run_im = scrape_ironman_official(race_name)\n",
    "            \n",
    "            # Use Ironman.com data if available\n",
    "            if bike_im and not bike_elevation:\n",
    "                bike_elevation = bike_im\n",
    "            if run_im and not run_elevation:\n",
    "                run_elevation = run_im\n",
    "        \n",
    "        # Cache the results\n",
    "        race_data_cache[race_name] = (bike_elevation, run_elevation)\n",
    "        logger.info(f\"Final data for {race_name}: Bike={bike_elevation}ft, Run={run_elevation}ft\")\n",
    "        \n",
    "        # Add a delay between processing races\n",
    "        time.sleep(1 + random.random() * 2)\n",
    "    \n",
    "    # Update the DataFrame with the collected data\n",
    "    update_count = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        race_name = row[\"Race\"]\n",
    "        \n",
    "        if not pd.isna(race_name) and race_name in race_data_cache:\n",
    "            bike_elev, run_elev = race_data_cache[race_name]\n",
    "            \n",
    "            # Update bike elevation if we found a value and the current cell is empty\n",
    "            if bike_elev is not None and pd.isna(row[\"bike elevation\"]):\n",
    "                df.at[idx, \"bike elevation\"] = bike_elev\n",
    "                update_count += 1\n",
    "                \n",
    "            # Update run elevation if we found a value and the current cell is empty\n",
    "            if run_elev is not None and pd.isna(row[\"run elevation\"]):\n",
    "                df.at[idx, \"run elevation\"] = run_elev\n",
    "                update_count += 1\n",
    "    \n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    output_path = \"S3_70.3_locations.csv_coord_elevat_updated.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    logger.info(f\"Updated {update_count} elevation values\")\n",
    "    logger.info(f\"Saved updated data to {output_path}\")\n",
    "    print(f\"‚úÖ Successfully updated {update_count} elevation values and saved to {output_path}\")\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
